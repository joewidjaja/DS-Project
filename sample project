library(data.table);
library(dplyr)
edu_local <- read.table(file = "C:/Users/F6F0AEN/Desktop/7406 Project/hsls_17_student_pets_sr_v1_0.csv", sep = ",", header=T);

edu <- edu_local[c("STU_ID","P1ABLEBA", "X1PAR1EDU","X1PAR1EMP","X2PAR2EDU","X2PAR2EMP","X1FAMINCOME","X2EVERDROP","X1LOCALE", "X1POVERTY","X2PARPATTERN", "X1REGION", "X1SEX")];
setnames(edu, old = c("X1PAR1EDU","X1PAR1EMP","X2PAR2EDU","X2PAR2EMP","X1FAMINCOME","X2EVERDROP","X1LOCALE","X1POVERTY","X2PARPATTERN","X1REGION","X1SEX"), new = c("PARENT1_EDU","PARENT1_EMPLOY","PARENT2_EDU","PARENT2_EMPLOY","FAMILY_INCOME","EVER_DROPOUT","SCHOOL_LOCALITY","POVERTY_INDICATOR","PARENT_RELATION","SCHOOL_REGION","STUDENT_SEX"));
str(edu)

# Some exploratory data analysis
dim(edu);
head(edu);
summary(edu);

# Drop all records where response variable is indeterminate
edu <- filter(edu,P1ABLEBA > 0);
graduate <- I(edu$P1ABLEBA <= 2);

View(edu)
View(graduate)
# drop STU_ID column
edu <- dplyr::select(edu, -c(STU_ID));  

edu <- data.frame(graduate, edu[,-1]); ## replace column "P1ABLEBA" by "graduate".

# Checking for missing column values
lapply(edu, function(x) sum(is.na(x)) );

# Splitting data into train and test sets
smp_size <- dim(edu)[1]*0.75;

## set the seed to make your partition reproducible
set.seed(123);
train_ind <- sample(seq_len(nrow(edu)), size = smp_size);

train <- edu[train_ind, ];
test <- edu[-train_ind, ];
dim(train);
dim(test);
trainModel <- train[c("graduate","PARENT1_EDU","PARENT1_EMPLOY","PARENT2_EDU","PARENT2_EMPLOY","FAMILY_INCOME","EVER_DROPOUT","SCHOOL_LOCALITY","POVERTY_INDICATOR","PARENT_RELATION","SCHOOL_REGION","STUDENT_SEX")];
testModel <- test[c("graduate", "PARENT1_EDU","PARENT1_EMPLOY","PARENT2_EDU","PARENT2_EMPLOY","FAMILY_INCOME","EVER_DROPOUT","SCHOOL_LOCALITY","POVERTY_INDICATOR","PARENT_RELATION","SCHOOL_REGION","STUDENT_SEX")];
#Histogram Plot of graduate Score
#hist(train$graduate, col = "green");

# GG Plot to show distribution of all variables
library(ggplot2);
library(purrr);
library(tidyr);
library(gridExtra);
pairs(train, upper.panel=NULL);

library(corrplot);
library(RColorBrewer);
M <-cor(edu);
corrplot(M, type="upper", order="hclust",col=brewer.pal(n=8, name="RdYlBu"));

library("PerformanceAnalytics");
#correlation plot, entire dataset
chart.Correlation(edu, histogram=TRUE, pch=19);
#variable

# Box Plots

box_PARENT1_EDU <- ggplot(edu, aes(y = PARENT1_EDU, x = graduate)) + geom_boxplot()
box_PARENT1_EMPLOY <- ggplot(edu, aes(y = PARENT1_EMPLOY,x = graduate)) + geom_boxplot()
box_PARENT2_EDU <- ggplot(edu, aes(y = PARENT2_EDU, x = graduate)) + geom_boxplot()
box_PARENT2_EMPLOY <- ggplot(edu, aes(y = PARENT2_EMPLOY,x = graduate)) + geom_boxplot()
box_FAMILY_INCOME <- ggplot(edu, aes(y=FAMILY_INCOME, x = graduate)) + geom_boxplot()
box_EVER_DROPOUT <- ggplot(edu, aes(y = EVER_DROPOUT, x = graduate)) + geom_boxplot()
box_SCHOOL_LOCALITY <- ggplot(edu, aes(y = SCHOOL_LOCALITY, x = graduate)) + geom_boxplot()
box_POVERTY_INDICATOR <- ggplot(edu, aes(y = POVERTY_INDICATOR, x = graduate)) + geom_boxplot()
box_PARENT_RELATION <- ggplot(edu, aes(y = PARENT_RELATION, x = graduate)) + geom_boxplot()
box_SCHOOL_REGION <- ggplot(edu, aes(y= SCHOOL_REGION, x = graduate)) + geom_boxplot()
box_STUDENT_SEX <- ggplot(edu, aes(y= STUDENT_SEX,x = graduate)) + geom_boxplot()

grid.arrange(box_PARENT1_EDU, box_PARENT1_EMPLOY, box_PARENT2_EDU, box_PARENT2_EMPLOY,
             box_FAMILY_INCOME, box_EVER_DROPOUT, box_SCHOOL_LOCALITY, box_POVERTY_INDICATOR,
             box_PARENT_RELATION, box_SCHOOL_REGION, box_STUDENT_SEX, ncol=3);
TrainErr <- NULL;   # This list contains the training errors for various models
TestErr  <- NULL;   # This list contains the testing errors for various models
		
#(1) LDA
# Fit the model
library(MASS);

mod1 <- lda(trainModel[,-1], trainModel[,1]); 
mod1;

# training error 
pred1 <- predict(mod1,trainModel[,-1])$class; 
TrainErr <- c(TrainErr, mean( pred1  != trainModel$graduate)); 
TrainErr;
# testing error 
pred1test <- predict(mod1,testModel[,-1])$class; 
TestErr <- c(TestErr,mean(pred1test != testModel$graduate));
TestErr;

table(pred1test,  testModel$graduate); 

#(2) QDA
mod2 <- qda(trainModel[,-1], trainModel[,1]); 
mod2;
# training Error 
pred2 <- predict(mod2,trainModel[,-1])$class;
TrainErr <- c(TrainErr, mean( pred2 != trainModel$graduate)); 
TrainErr;
# testing Error 
pred2test <- predict(mod2,testModel[,-1])$class; 
TestErr <- c(TestErr, mean( pred2test !=  testModel$graduate))
TestErr;

table(pred2test,  testModel$graduate);

#(3) Naive Bayes
library(e1071)
mod3 <- naiveBayes(trainModel[,-1], trainModel[,1]); 
mod3;
# training Error
pred3 <- predict(mod3, trainModel[,-1]);
TrainErr <- c(TrainErr, mean( pred3 != trainModel$graduate));
TrainErr; 
# testing Error 
pred3test <- predict(mod3,testModel[,-1]); 
TestErr <- c(TestErr,  mean( pred3test != testModel$graduate));
TestErr;

table(pred3test,  testModel$graduate);

#(4) Logistic Regression
library(nnet);
#fmla <- as.formula('graduate ~ X1PAR1EDU+X1PAR1EMP+X1SEX+X1REGION+X1FAMINCOME+X2EVERDROP+X1LOCALE+X1POVERTY+X2PARPATTERN')
modlogistic <- glm( graduate ~ ., family = binomial(link = "logit"), data= trainModel);

mod4 <- multinom(graduate ~., data=trainModel);

summary(modlogistic)
summary(mod4);
# training error
TrainErr <- c(TrainErr, mean( predict(mod4, trainModel[,-1]) != trainModel$graduate));
TrainErr;
# testing error 
pred4test <- predict(mod4,testModel[,-1]); 
TestErr <- c(TestErr,  mean( pred4test != testModel$graduate));
TestErr;

table(pred4test,  testModel$graduate);

#AIC (optional)
backwards = step(mod4)
formula(backwards)
summary(backwards)
backwards$anova

#default step is both direction
step <- stepAIC(mod4,trace = FALSE)
step$anova

BIC <- stepAIC(mod4,k=log(nrow(trainModel)))

library(MASS)
step.model <- mod4 %>% stepAIC(trace = FALSE)
coef(step.model)

# (5)KNN
library(class);
x <- c(1,3,5,7,9,11,13,15,17,19);

## KNN on test data with k=1,3,5,7,9,11,13,15,17,19
for (val in x) {
  xnew <- trainModel[,-1];
  ypred.train <- knn(trainModel[,-1], xnew, trainModel$graduate, k=val);
  kk_mean <- mean( ypred.train != trainModel$graduate);
  print (val);
  print (kk_mean);
}

## KNN on test data with k=1,3,5,7,9,11,13,15,17,19
x <- c(1,3,5,7,9,11,13,15,17,19);
for (val in x) {
  xnew2 <- testModel[,-1];
  ypred.test <- knn(trainModel[,-1], xnew2, trainModel$graduate, k=val);
  kk_mean <- mean( ypred.test != testModel$graduate)
  print (val)
  print (kk_mean)
}

##==================================================================##
require(class)
set.seed(1)
acc <- list()

x_train <- trainModel [,c("PARENT1_EDU","PARENT1_EMPLOY","PARENT2_EDU","PARENT2_EMPLOY","FAMILY_INCOME","EVER_DROPOUT","SCHOOL_LOCALITY","POVERTY_INDICATOR","PARENT_RELATION","SCHOOL_REGION","STUDENT_SEX")]
y_train <- trainModel$graduate
x_test <- testModel[,c("PARENT1_EDU","PARENT1_EMPLOY","PARENT2_EDU","PARENT2_EMPLOY","FAMILY_INCOME","EVER_DROPOUT","SCHOOL_LOCALITY","POVERTY_INDICATOR","PARENT_RELATION","SCHOOL_REGION","STUDENT_SEX")]

for (i in 1:20) {
  knn_pred <- knn(train = x_train, test = x_test, cl = y_train, k = i)
  acc[as.character(i)] = mean(knn_pred == testModel$graduate)
}
acc <- unlist(acc)

data_frame(acc = acc) %>%
  mutate(k = row_number()) %>%
  ggplot(aes(k, acc)) +
  geom_col(aes(fill = k == which.max(acc))) +
  labs(x = 'K', y = 'Accuracy', title = 'KNN Accuracy for different values of K') +
  scale_x_continuous(breaks = 1:20) +
  scale_y_continuous(breaks = round(c(seq(0.50, 0.95,0.01), max(acc)),
                                    digits = 3)) +
  geom_hline(yintercept = max(acc), lty = 2) +
  coord_cartesian(ylim = c(min(acc), max(acc))) +
  guides(fill = FALSE)
#Based on the graph, the best KNN is 3 with accuracy of 0.964
# (6)PCA-KNN

library(pls);
trainpca <- prcomp(trainModel[,-1]);  
trainpca$sdev;
round(trainpca$sdev,2);
matplot(2:10, trainpca$rot[,1:3], type ="l", xlab="", ylab="");
matplot(2:10, trainpca$rot[,1:5], type ="l", xlab="", ylab="");
plot(trainpca$sdev,type="l", ylab="SD of PC", xlab="PC number");
modelpca <- lm(graduate ~ trainpca$x[,1:3], data = trainModel);
#modelpca <- lm(graduate ~ trainpca$x[,1:3], data = trainpca);
modelpca;
summary(modelpca);

pred5test <- predict(modelpca,testModel[,-1]); 

edu.pca <- pcr(graduate~., data=trainModel, validation="CV");  
validationplot(edu.pca);
summary(edu.pca); 

## The minimum 
ncompopt <- which.min(edu.pca$validation$adj);
## 
## 6B(iv) Training Error with the optimal choice of PCs
ypred6train <- predict(edu.pca, ncomp = ncompopt, newdata = trainModel[-1]); 
TrainErr <- c(TrainErr,mean( (ypred6train - trainModel$graduate)^2)); 
TrainErr; 

## 6B(v) Testing Error with the optimal choice of PCs
ypred6test <- predict(edu.pca, ncomp = ncompopt, newdata = testModel[-1]); 
TestErr <- c(TestErr,mean( (ypred6test - testModel$graduate)^2)); 
TestErr 

#(7a) The decision tree
library(rpart)
library(rpart.plot)
rpart_model <- rpart(graduate ~ ., data = trainModel,parms = list(split = "gini"))
summary(rpart_model)
rpart.plot(rpart_model)

#Training Error
pred7 <- predict(rpart_model, newdata = trainModel)
TrainErr <- c(TrainErr,mean(pred7 != trainModel$graduate))
TrainErr

# Testing error
pred7test <- predict(rpart_model, newdata = testModel)
TestErr <- c(TestErr,mean(pred7test != testModel$graduate))
TestErr

table(pred7test,testModel$graduate);

#(7b) The prune decision tree
# Optimal tree size
plotcp(rpart_model)
printcp(rpart_model)
#Based on comparison, optimal CP is the 4th row with the optimal CP=0.01. 
#Since we wanted to compare with the previous tree model, so we could use CP in the 3rd row as sub-optimal prune tree
cp2 = rpart_model$cptable[3,"CP"]
rpart_prune <- rpart(graduate ~ ., data = trainModel,control = rpart.control(cp = cp2))
rpart_prune
rpart.plot(rpart_prune)

#Training Error
pred7b <- predict(rpart_prune, newdata = trainModel)
TrainErr <- c(TrainErr,mean(pred7b != trainModel$graduate))
TrainErr

# Testing error
pred7btest <- predict(rpart_prune, newdata = testModel)
TestErr <- c(TestErr,mean(pred7btest != testModel$graduate))
TestErr
table(pred7btest,testModel$graduate);

# the unpruned tree has a smaller training error
## The unpruned tree or the tree with the optimal CP value is actually
## having a larger testing errors than those sub-optimal CP value

#(8) Random Forest
set.seed(123)
library(randomForest)
rf_mod <- randomForest(as.factor(graduate) ~., data=trainModel,importance=TRUE)

rf_mod
importance(rf_mod)
importance(rf_mod,type = 2)
varImpPlot(rf_mod)
#EVERDROP, POVERTY,SEX, FAMINCOME AND LOCALE are the most important variables when predicting graduate 

#Training Error
y1<- trainModel$graduate
pred8 <- predict(rf_mod, trainModel)
TrainErr <- c(TrainErr, mean( pred8 != y1));
TrainErr;

#Testing Error
y2 <- testModel$graduate
pred8test <- predict(rf_mod,testModel,type = 'class')
TestErr <- c(TestErr,mean(pred8test != y2));
TestErr

table(pred8test,testModel$graduate);
